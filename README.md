# Multi Agent Cybersecurity System

## 1 Запуск моделей из Ollama на локалке

### Установка Ollama
```
curl -fsSL https://ollama.com/install.sh | sh
```

### Запуск пустого сервера
```
ollama serve
```

### Загрузка моделей для инференса
```
ollama pull llama3.1
```

Сейчас в качестве оркестратора (управляющей LLM) выбрана llama3.1 (~5 гб видеопамяти) - она разворачивается вручную на ollama
Также для RAG в website search tool нужна эмбеддинговая модель (маленькая модель, которая просто переводит текст в вектора): для этого выбрана nomic-embed-text - также из Ollama
Модели можно поменять, покопавшись в https://ollama.com/search

### Посмотреть какие модели работают на локалке
```
ollama list
```

### Проблемы с моделями на Ollama

- gemma3 - нет поддержки тулов
- qwen3 - странный встроенный ReAct, в output встроен блок think, которому там быть не следует
- deepseek-r1 - конченный встроенный ReAct и вобще невозможно встраивать свои тулы
- mistral - вместо адекватного текстового ответа выдает код на Python по генерации ответа, может проблема в плохом промпте
